{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENSURE THAT THE OPENSEARCH SERVICE (AND OPTIONALLY OPENSEARCH-DASHBOARDS) IN docker-compose.yml\n",
    "### IS UNCOMMENTED AND RUNNING IF YOU WANT TO USE THE CONTAINERIZED INSTANCES\n",
    "\n",
    "# https://python.langchain.com/v0.2/docs/integrations/llms/llamacpp/#usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from textwrap import dedent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENSEARCH_HOST = \"localhost\" # When running notebook outside of compoose jupyter container\n",
    "OPENSEARCH_HOST = \"opensearch\" # When running notebook inside of compose jupyter container\n",
    "OPENSEARCH_PORT = 9200\n",
    "OPENSEARCH_HTTP_URL= f\"http://{OPENSEARCH_HOST}:{OPENSEARCH_PORT}\"\n",
    "OPENSEARCH_USER = \"admin\"\n",
    "OPENSEARCH_PASSWORD = \"admin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings_model_name = \"thenlper/gte-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"/workspace/data/state_of_the_union.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    engine=\"faiss\",\n",
    "    space_type=\"innerproduct\",\n",
    "    ef_construction=256,\n",
    "    m=48,\n",
    "    opensearch_url=OPENSEARCH_HTTP_URL,\n",
    "    http_auth=(OPENSEARCH_USER, OPENSEARCH_PASSWORD),\n",
    "    use_ssl = False,\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much does the president want to cut the cancer death rate?\"\n",
    "docs = docsearch.similarity_search(query, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n",
    "# model_name = \"TheBloke/CollectiveCognition-v1.1-Mistral-7B-GPTQ\"\n",
    "MODEL_NAME=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\"\n",
    "MODEL_FILE=\"Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\"\n",
    "model_file_path = hf_hub_download(repo_id=MODEL_NAME, filename=MODEL_FILE)\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_file_path,\n",
    "    temperature=0.75,\n",
    "    max_tokens=75,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    "    n_gpu_layers=33\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model without any additional context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much does the president want to cut the cancer death rate?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model with retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_retrieval_qa(query, k=4, max_new_tokens=20, return_sources=True, repetition_penalty=1.0, remove_tokens=(\"<s>\",\"</s>\"), verbose=False):\n",
    "    \n",
    "    global llm\n",
    "    \n",
    "    docs = docsearch.similarity_search(query, k=k)\n",
    "\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    template = PromptTemplate.from_template(f\"\"\"\"\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Answer:\"\"\")\n",
    "\n",
    "    llm.max_tokens = max_new_tokens\n",
    "    # llm = LlamaCpp(\n",
    "    #     model_path=model_file_path,\n",
    "    #     temperature=0.75,\n",
    "    #     max_tokens=max_new_tokens,\n",
    "    #     top_p=1,\n",
    "    #     callback_manager=callback_manager,\n",
    "    #     verbose=verbose,  # Verbose is required to pass to the callback manager\n",
    "    #     # n_gpu_layers=-1\n",
    "    # )\n",
    "\n",
    "    chain = template | llm\n",
    "\n",
    "    ## The batch_decode call below removes the input tokens\n",
    "    generated_text = chain.invoke({\"context\":context, \"query\":query})\n",
    "    \n",
    "    for token in remove_tokens:\n",
    "        generated_text = generated_text.replace(token,\"\")\n",
    "    generated_text = generated_text.strip('\" \\n')\n",
    "    \n",
    "    output = {\n",
    "        \"text\": generated_text\n",
    "    }\n",
    "    if return_sources: output[\"sources\"] = docs\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_retrieval_qa(query, max_new_tokens=120, repetition_penalty=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Multiple Vector Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a \"summary\" field for the documents we want to index, so we will have both a \"page_content\" field and a \"summary\" field to embed and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearch_docs = [doc.dict() for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text:str, max_new_tokens=200, repetition_penalty=1.1):\n",
    "    \n",
    "    summarization_template_string = \"\"\"Summarize the following information. Capture the important information, but be as concise as possible.\n",
    "\n",
    "    Information: {document}\n",
    "\n",
    "    Summary: \"\"\"\n",
    "    summarization_template = PromptTemplate.from_template(summarization_template_string)\n",
    "\n",
    "    llm.max_tokens = max_new_tokens\n",
    "\n",
    "    chain = summarization_template | llm\n",
    "\n",
    "    ## The batch_decode call below removes the input tokens\n",
    "    generated_text = chain.invoke(text)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in opensearch_docs:\n",
    "    text = doc[\"page_content\"]\n",
    "    summary = summarize_text(text)\n",
    "    doc[\"summary\"] = summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings for the \"page_content\" and \"summary\" fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text:str, embedding_model=embeddings):\n",
    "    return embedding_model.embed_documents(text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in opensearch_docs:\n",
    "    for field in [\"page_content\", \"summary\"]:\n",
    "        doc[f\"{field}_vector\"] = generate_embeddings(doc[field])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Documents to Opensearch Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"my-multi-vector-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "from hashlib import sha1\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenSearch(\n",
    "    hosts=OPENSEARCH_HTTP_URL,\n",
    "    http_auth=(OPENSEARCH_USER, OPENSEARCH_PASSWORD),\n",
    "    use_ssl=False,\n",
    "    verify_certs=False,\n",
    "    ssl_assety_hostname=False,\n",
    "    ssl_show_warn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(index, mappings={}, opensearch_client=client, replace_existing=False, number_of_shards=1):\n",
    "    \n",
    "    if replace_existing:\n",
    "        opensearch_client.indices.delete(index, ignore_unavailable=True)\n",
    "        print(f\"Deleted existing index: {index}\")\n",
    "\n",
    "    index_body = {\n",
    "        'settings': {\n",
    "            'index': {\n",
    "                'knn': True,\n",
    "                \"knn.algo_param.ef_search\": 256,\n",
    "                'number_of_shards':number_of_shards\n",
    "            }\n",
    "        },\n",
    "        \"mappings\" : mappings\n",
    "    }\n",
    "    response = opensearch_client.indices.create(index=index_name,body=index_body)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    \"properties\" :  {\n",
    "        \"page_content_vector\" : {\n",
    "            \"type\" : \"knn_vector\",\n",
    "            \"dimension\": embeddings.client.get_sentence_embedding_dimension(),\n",
    "            \"method\": {\n",
    "            \"name\": \"hnsw\"\n",
    "          }\n",
    "        },\n",
    "        \"summary_vector\" : {\n",
    "            \"type\" : \"knn_vector\",\n",
    "            \"dimension\": embeddings.client.get_sentence_embedding_dimension(),\n",
    "            \"method\": {\n",
    "            \"name\": \"hnsw\"\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "create_index(index=index_name, mappings=mappings, replace_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_actions_string = \"\\n\".join([\n",
    "f\"\"\"{json.dumps({\"index\": {\n",
    "                \"_index\":index_name,\n",
    "                 \"_id\":sha1(doc[\"page_content\"].encode()).hexdigest()\n",
    "                 }})}\n",
    "{json.dumps(doc)}\n",
    "\"\"\" for doc in opensearch_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.bulk(bulk_actions_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_field_search(query:str, field_name:str, k=3, size=3, index=index_name, opensearch_client=client, embedding_model=embeddings):\n",
    "    \"\"\"k is the number of neighbors the search of each graph will return. You must also include the size option, which indicates how many results the query actually returns. \n",
    "    The plugin returns k amount of results for each shard (and each segment) and size amount of results for the entire query. The plugin supports a maximum k value of 10,000.\n",
    "    \"\"\"\n",
    "    query_body = {\n",
    "        \"size\": size,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                field_name : {\n",
    "                    \"vector\": generate_embeddings(text=query, embedding_model=embedding_model),\n",
    "                    \"k\": k\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # query_body = {\n",
    "    #     \"size\": size,\n",
    "    #     \"query\": {\n",
    "    #     \"script_score\": {\n",
    "    #         \"query\": {\n",
    "    #         \"match_all\": {}\n",
    "    #         },\n",
    "    #         \"script\": {\n",
    "    #         \"source\": \"knn_score\",\n",
    "    #         \"lang\": \"knn\",\n",
    "    #         \"params\": {\n",
    "    #             \"field\": field_name,\n",
    "    #             \"query_value\": generate_embeddings(text=query, embedding_model=embedding_model),\n",
    "    #             \"space_type\": \"cosinesimil\"\n",
    "    #         }\n",
    "    #         }\n",
    "    #     }\n",
    "    #     }\n",
    "    #     }\n",
    "\n",
    "    return opensearch_client.search(index=index, body=query_body)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_field_search(query=\"International supply chain\", field_name=\"page_content_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k:v for k,v in results[\"hits\"][\"hits\"][0][\"_source\"].items() if k in [\"page_content\",\"summary\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BELOW STILL IN PROGRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval with Reranking (Boosting with Text Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenSearch(\n",
    "    hosts=OPENSEARCH_HTTPS_URL,\n",
    "    http_auth=(OPENSEARCH_USER, OPENSEARCH_PASSWORD),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assety_hostname=False,\n",
    "    ssl_show_warn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_query = {\n",
    "  'size': 5,\n",
    "  'query': {\n",
    "    'multi_match': {\n",
    "      'query': query,\n",
    "    #   'fields': ['title^2', 'director']\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_res = client.search(os_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os_res[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
