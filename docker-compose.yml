services:
  jupyter:
    image: generative-ai-jupyter-cpu
    build: 
      context: ./jupyter
      args:
        - PROCESSOR=cpu
    ports: 
      - 8888:8888
    volumes:
      - ./data:/workspace/data
      - ./jupyter/notebooks/:/workspace/notebooks/
      - ./.env:/workspace/.env
      - ~/.cache/:/root/.cache/
  chroma:
    image: chromadb/chroma
    command: uvicorn chromadb.app:app --reload --workers 1 --host 0.0.0.0 --port 8000 --log-config chromadb/log_config.yml
    environment:
      - IS_PERSISTENT=TRUE
      - PERSIST_DIRECTORY=${PERSIST_DIRECTORY:-/chroma/chroma}
      - ANONYMIZED_TELEMETRY=False
      # - CHROMA_SERVER_AUTH_PROVIDER=${CHROMA_SERVER_AUTH_PROVIDER}
      # - CHROMA_SERVER_AUTH_CREDENTIALS_FILE=${CHROMA_SERVER_AUTH_CREDENTIALS_FILE}
      # - CHROMA_SERVER_AUTH_CREDENTIALS=${CHROMA_SERVER_AUTH_CREDENTIALS}
      # - CHROMA_SERVER_AUTH_CREDENTIALS_PROVIDER=${CHROMA_SERVER_AUTH_CREDENTIALS_PROVIDER}
      # - CHROMA_OTEL_EXPORTER_ENDPOINT=${CHROMA_OTEL_EXPORTER_ENDPOINT}
      # - CHROMA_OTEL_EXPORTER_HEADERS=${CHROMA_OTEL_EXPORTER_HEADERS}
      # - CHROMA_OTEL_SERVICE_NAME=${CHROMA_OTEL_SERVICE_NAME}
      # - CHROMA_OTEL_GRANULARITY=${CHROMA_OTEL_GRANULARITY}
    ports:
      - 8000:8000
    volumes:
      - chroma-data:/chroma
    healthcheck:
      test: ["CMD","curl", "http://localhost:8000/api/v1/heartbeat", "-H", "'accept: application/json'"]
      interval: 1m30s
      timeout: 30s
      retries: 5
      start_period: 30s
  
  frontend:
    build: ./frontend
    # environment:
    #   - name=value
    ports: 
      - 9000:9000
    volumes:
      - ./frontend/src/:/app/src/
  
  # streamlit:
  #   build: 
  #     context: ./streamlit_app/
  #   environment:
  #     - CHROMADB_HOST=chroma
  #     - CHROMADB_PORT=8000
  #   ports:
  #     - 8501:8501
  #   volumes:
  #     - ./streamlit_app/:/app/
  #     - ~/.cache/:/root/.cache/

  inference-api:
    image: generative-ai-inference-api-cpu
    build: 
      context: ./api
      args:
        - PROCESSOR=cpu
    ports: 
      - 5000:5000
    environment:
      - MODEL_NAME=TheBloke/Mistral-7B-Instruct-v0.2-GGUF
      - MODEL_FILE=mistral-7b-instruct-v0.2.Q4_K_M.gguf
      - TOKENIZER_MODEL_NAME=TheBloke/Mistral-7B-Instruct-v0.2-GPTQ
      - MODEL_TYPE=GGUF
      - N_GPU_LAYERS=0
      - CONTEXT_LENGTH=8192
      - LLAMA_CPP_THREADS=4
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:5000/api/health']
      start_period: 7s
    restart: always
    volumes:
      - ./api/src/:/app/src/
      - ./.env:/workspace/.env
      - ~/.cache/:/home/fastapi/.cache/

  opensearch: # This is also the hostname of the container within the Docker network (i.e. https://opensearch-node1/)
    image: opensearchproject/opensearch:latest # Specifying the latest available image - modify if you want a specific version
    environment:
      # - OPENSEARCH_INITIAL_ADMIN_PASSWORD=El@sticSearch123
      - DISABLE_SECURITY_PLUGIN=true
      - DISABLE_INSTALL_DEMO_CONFIG=true
      - discovery.type=single-node
      - bootstrap.memory_lock=true # Disable JVM heap memory swapping
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx1g" # Set min and max JVM heap sizes to at least 50% of system RAM
    ulimits:
      memlock:
        soft: -1 # Set memlock to unlimited (no soft or hard limit)
        hard: -1
      nofile:
        soft: 65536 # Maximum number of open files for the opensearch user - set to at least 65536
        hard: 65536
    volumes:
      - opensearch-data:/usr/share/opensearch/data # Creates volume called opensearch-data1 and mounts it to the container
    ports:
      - 9200:9200 # REST API
      - 9600:9600 # Performance Analyzer
  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:latest # Make sure the version of opensearch-dashboards matches the version of opensearch installed on other nodes
    ports:
      - 5601:5601 # Map host port 5601 to container port 5601
    expose:
      - "5601" # Expose port 5601 for web access to OpenSearch Dashboards
    environment:
      OPENSEARCH_HOSTS: '["http://opensearch:9200"]' # Define the OpenSearch nodes that OpenSearch Dashboards will query
      DISABLE_SECURITY_DASHBOARDS_PLUGIN: true
  
  # elasticsearch:
  #   image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0 #7.17.16
  #   restart: always
  #   environment:
  #     - xpack.security.enabled=false
  #     - discovery.type=single-node
  #   ulimits:
  #     memlock:
  #       soft: -1
  #       hard: -1
  #     nofile:
  #       soft: 65536
  #       hard: 65536
  #   volumes:
  #     - elasticsearch-data:/usr/share/elasticsearch/data
  #   ports:
  #     - 9200:9200
  # kibana:
  #   image: docker.elastic.co/kibana/kibana:8.12.0 #7.17.16
  #   restart: always
  #   environment:
  #     - ELASTICSEARCH_HOSTS=http://elasticsearch:9200    # address of elasticsearch docker container which kibana will connect
  #   ports:
  #     - 5601:5601
  #   depends_on:
  #     - elasticsearch                                   # kibana will start when elasticsearch has started
  
  # text-generation-inference:
  #   image: ghcr.io/huggingface/text-generation-inference
  #   ports:
  #     - 3000:3000
  #   command: ["--model-id", "TheBloke/Mistral-7B-Instruct-v0.2-GPTQ", "--revision", "gptq-8bit-128g-actorder_True", "--port", "3000", "--quantize", "gptq", "--max-input-length", "3696", "--max-total-tokens", "4096", "--max-batch-prefill-tokens", "4096"]
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   volumes:
  #     - tgi-data:/data

volumes:
  chroma-data:
  # elasticsearch-data:
  opensearch-data:
  tgi-data:
  