version: '3.9'

services:
  jupyter:
    image: generative-ai-jupyter-cpu
    build: 
      context: ./jupyter
      args:
        - PROCESSOR=cpu
    ports: 
      - 8888:8888
    volumes:
      - ./data:/workspace/data
      - ./jupyter/notebooks/:/workspace/notebooks/
      - ./.env:/workspace/.env
      - ~/.cache/:/root/.cache/
  chroma:
    image: chromadb/chroma
    command: uvicorn chromadb.app:app --reload --workers 1 --host 0.0.0.0 --port 8000 --log-config chromadb/log_config.yml
    environment:
      - IS_PERSISTENT=TRUE
      - PERSIST_DIRECTORY=${PERSIST_DIRECTORY:-/chroma/chroma}
      - ANONYMIZED_TELEMETRY=False
      # - CHROMA_SERVER_AUTH_PROVIDER=${CHROMA_SERVER_AUTH_PROVIDER}
      # - CHROMA_SERVER_AUTH_CREDENTIALS_FILE=${CHROMA_SERVER_AUTH_CREDENTIALS_FILE}
      # - CHROMA_SERVER_AUTH_CREDENTIALS=${CHROMA_SERVER_AUTH_CREDENTIALS}
      # - CHROMA_SERVER_AUTH_CREDENTIALS_PROVIDER=${CHROMA_SERVER_AUTH_CREDENTIALS_PROVIDER}
      # - CHROMA_OTEL_EXPORTER_ENDPOINT=${CHROMA_OTEL_EXPORTER_ENDPOINT}
      # - CHROMA_OTEL_EXPORTER_HEADERS=${CHROMA_OTEL_EXPORTER_HEADERS}
      # - CHROMA_OTEL_SERVICE_NAME=${CHROMA_OTEL_SERVICE_NAME}
      # - CHROMA_OTEL_GRANULARITY=${CHROMA_OTEL_GRANULARITY}
    ports:
      - 8000:8000
    volumes:
      - chroma-data:/chroma
    healthcheck:
      test: ["CMD","curl", "http://localhost:8000/api/v1/heartbeat", "-H", "'accept: application/json'"]
      interval: 1m30s
      timeout: 30s
      retries: 5
      start_period: 30s
  
  streamlit:
    build: 
      context: ./streamlit_app/
    environment:
      - CHROMADB_HOST=chroma
      - CHROMADB_PORT=8000
    ports:
      - 8501:8501
    volumes:
      - ./streamlit_app/:/app/
      - ~/.cache/:/root/.cache/
  inference-api:
    image: generative-ai-inference-api-cpu
    build: 
      context: ./api
      args:
        - PROCESSOR=cpu
    ports: 
      - 5000:5000
    environment:
      - MODEL_NAME=TheBloke/Mistral-7B-Instruct-v0.2-GGUF
      - MODEL_FILE=mistral-7b-instruct-v0.2.Q4_K_M.gguf
      - MODEL_TYPE=GGUF
      - TOKENIZER_MODEL_NAME=TheBloke/Mistral-7B-Instruct-v0.2-GPTQ
      - N_GPU_LAYERS=0
      - CONTEXT_LENGTH=8192
      - LLAMA_CPP_THREADS=4
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:5000/api/health']
      start_period: 7s
    restart: always
    volumes:
      - ./api/src/:/app/src/
      - ./.env:/workspace/.env
      - ~/.cache/:/home/fastapi/.cache/
  
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0 #7.17.16
    restart: always
    environment:
      - xpack.security.enabled=false
      - discovery.type=single-node
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
  kibana:
    image: docker.elastic.co/kibana/kibana:8.12.0 #7.17.16
    restart: always
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200    # address of elasticsearch docker container which kibana will connect
    ports:
      - 5601:5601
    depends_on:
      - elasticsearch                                   # kibana will start when elasticsearch has started
  
  # text-generation-inference:
  #   image: ghcr.io/huggingface/text-generation-inference
  #   ports:
  #     - 3000:3000
  #   command: ["--model-id", "TheBloke/Mistral-7B-Instruct-v0.2-GPTQ", "--revision", "gptq-8bit-128g-actorder_True", "--port", "3000", "--quantize", "gptq", "--max-input-length", "3696", "--max-total-tokens", "4096", "--max-batch-prefill-tokens", "4096"]
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   volumes:
  #     - tgi-data:/data

volumes:
  chroma-data:
  elasticsearch-data:
  tgi-data:
  