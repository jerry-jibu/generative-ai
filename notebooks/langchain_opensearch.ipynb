{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from textwrap import dedent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENSEARCH_HOST = \"localhost\" # When running notebook outside of compoose jupyter container\n",
    "OPENSEARCH_HOST = \"opensearch\" # When running notebook inside of compose jupyter container\n",
    "OPENSEARCH_PORT = 9200\n",
    "OPENSEARCH_HTTPS_URL= f\"https://{OPENSEARCH_HOST}:{OPENSEARCH_PORT}\"\n",
    "OPENSEARCH_USER = \"admin\"\n",
    "OPENSEARCH_PASSWORD = \"admin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model_name = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = TextLoader(\"../data/state_of_the_union.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    engine=\"faiss\",\n",
    "    space_type=\"innerproduct\",\n",
    "    ef_construction=256,\n",
    "    m=48,\n",
    "    opensearch_url=OPENSEARCH_HTTPS_URL,\n",
    "    http_auth=(OPENSEARCH_USER, OPENSEARCH_PASSWORD),\n",
    "    use_ssl = False,\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much does the president want to cut the cancer death rate?\"\n",
    "docs = docsearch.similarity_search(query, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Last month, I announced our plan to supercharge  \\nthe Cancer Moonshot that President Obama asked me to lead six years ago. \\n\\nOur goal is to cut the cancer death rate by at least 50% over the next 25 years, turn more cancers from death sentences into treatable diseases.  \\n\\nMore support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \\n\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTANT NOTE If you want to use the Mistral 7B Model\n",
    "## As of 10/7/2023, need to run the pip install below, as Mistral is not included in main transformers library yet\n",
    "# !pip install git+https://github.com/huggingface/transformers.git\n",
    "\n",
    "## RESTART THE KERNEL AFTER INSTALLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n",
    "# model_name = \"TheBloke/CollectiveCognition-v1.1-Mistral-7B-GPTQ\"\n",
    "model_name = \"TheBloke/Llama-2-7B-chat-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             device_map=\"auto\",\n",
    "                                             )\n",
    "# model=model.to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          device_map=\"auto\"\n",
    "                                          )\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model without any additional context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much does the president want to cut the cancer death rate?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(query, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_tensor = model.generate(inputs, min_new_tokens=25, max_new_tokens=100, repetition_penalty=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,  1128,  1568,   947,   278,  6673,   864,   304,  5700,   278,\n",
       "        23900,  4892,  6554, 29973,    13,    13,  1576,  7178, 29915, 29879,\n",
       "         1815,  2265,  4546,   787,  8711, 14511,  1230,   263,  9893,   304,\n",
       "        10032,   278,  1353,   310, 23900,  4892, 29879,   491, 29871, 29945,\n",
       "        29900, 29995,   975,   278,  2446,   316,  6332, 29889,   910,   626,\n",
       "         2966,  2738,  7306,   674,  1996,  7282, 13258,  1860,   297,  5925,\n",
       "        29892,   848, 19383, 29892,   322,   716,  7539,  1860, 29892,   408,\n",
       "         1532,   408,  3620,   297,  9045, 18020, 24833,   322, 23274, 29889,\n",
       "          450,  8037,  5619,   756,  7972,   395, 29896, 24464,   297,  5220,\n",
       "          292,   363,   278,  1824, 29892,   607,   338,  3806,   304,   367,\n",
       "        19228,   491,  2024,  1016,   943,   322,   916,  8974,   310, 29199,\n",
       "        29889,     2], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the token ID's for the generated response\n",
    "output_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decode the tokens like this (instead of decoding the entire output_tensor)\n",
    "## so that we don't include the propmt itself in the output\n",
    "generated_text = tokenizer.batch_decode(output_tensor[:, inputs.shape[1]:])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The President's Cancer Moonshot initiative aims to reduce the number of cancer deaths by 50% over the next decade. This ambitious goal will require significant investments in research, data sharing, and new treatments, as well as changes in healthcare policies and practices. The White House has proposed $1 billion in funding for the program, which is expected to be matched by private donors and other sources of funds.</s>\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model with retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_retrieval_qa(query, k=4, min_new_tokens=1, max_new_tokens=20, return_sources=True, repetition_penalty=1.0, remove_tokens=(\"<s>\",\"</s>\")):\n",
    "    docs = docsearch.similarity_search(query, k=k)\n",
    "\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    template = PromptTemplate(template=dedent(f\"\"\"\"\n",
    "    <SYS>You are a question-and-answer assistant that only uses information from the provided context when responding.</SYS>\n",
    "    \n",
    "    <s>[INST] Use the context to answer the question. Be detailed and specific in your answers, but do not include anything besides the answer to the question in your response.\n",
    "    \n",
    "    Context: {context}\n",
    "\n",
    "    Question: {{query}}[/INST]\n",
    "\n",
    "    Answer: \n",
    "    \"\"\"), input_variables=[\"query\"])\n",
    "\n",
    "    prompt = template.format(query=query)\n",
    "\n",
    "    input_tensor = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output_tensor = model.generate(input_tensor, min_new_tokens=min_new_tokens, max_new_tokens=max_new_tokens, repetition_penalty=repetition_penalty)\n",
    "\n",
    "    ## The batch_decode call below removes the input tokens\n",
    "    generated_text = tokenizer.batch_decode(output_tensor[:, input_tensor.shape[1]:])[0]\n",
    "    \n",
    "    for token in remove_tokens:\n",
    "        generated_text = generated_text.replace(token,\"\")\n",
    "    generated_text = generated_text.strip('\" \\n')\n",
    "    \n",
    "    output = {\n",
    "        \"text\": generated_text\n",
    "    }\n",
    "    if return_sources: output[\"sources\"] = docs\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_retrieval_qa(query, min_new_tokens=150, max_new_tokens=300, repetition_penalty=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The president wants to cut the cancer death rate by at least 50% over the next 25 years. This is a key component of the Cancer Moonshot initiative that he launched six years ago. The goal is to turn more cancers from death sentences into treatable diseases and provide more support for patients and families affected by cancer. To achieve this goal, the president has called on Congress to fund ARPA-H, an agency dedicated to driving breakthroughs in cancer research. By investing in research and development, the president believes that the United States can make significant progress in reducing cancer mortality rates and improving the lives of millions of Americans.\n"
     ]
    }
   ],
   "source": [
    "print(res[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Multiple Vector Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a \"summary\" field for the documents we want to index, so we will have both a \"page_content\" field and a \"summary\" field to embed and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearch_docs = [doc.dict() for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text:str, min_new_tokens=1, max_new_tokens=200, repetition_penalty=1.1):\n",
    "    \n",
    "    summarization_template_string = \"\"\"[SYS]You are an assistant with a great ability for summarizing text content.[/SYS]\n",
    "    \n",
    "    <s>[INST]Summarize the following information. Capture the important information, but be as concise as possible.\n",
    "\n",
    "    Information: {document}[/INST]\n",
    "\n",
    "    Summary: \"\"\"\n",
    "    summarization_template = PromptTemplate(template=summarization_template_string, input_variables=[\"document\"])\n",
    "    \n",
    "    summarization_prompt = summarization_template.format(document=text)\n",
    "\n",
    "    input_tensor = tokenizer.encode(summarization_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output_tensor = model.generate(\n",
    "                                   input_tensor, \n",
    "                                   min_new_tokens=min_new_tokens, \n",
    "                                   max_new_tokens=max_new_tokens, \n",
    "                                   repetition_penalty=repetition_penalty\n",
    "                                  )\n",
    "\n",
    "    generated_text = tokenizer.batch_decode(output_tensor[:, input_tensor.shape[1]:])[0]\n",
    "    \n",
    "\n",
    "    try:\n",
    "        generated_text = generated_text.split(\"Summary: \")[1]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    replace_tokens = (\"<s>\",\"</s>\")\n",
    "    for token in replace_tokens:\n",
    "        generated_text = generated_text.replace(token,\"\")\n",
    "    generated_text = generated_text.strip()\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in opensearch_docs:\n",
    "    text = doc[\"page_content\"]\n",
    "    summary = summarize_text(text)\n",
    "    doc[\"summary\"] = summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings for the \"page_content\" and \"summary\" fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text:str, embedding_model=embeddings):\n",
    "    return embedding_model.embed_documents(text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in opensearch_docs:\n",
    "    for field in [\"page_content\", \"summary\"]:\n",
    "        doc[f\"{field}_vector\"] = generate_embeddings(doc[field])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Documents to Opensearch Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"my-multi-vector-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "from hashlib import sha1\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenSearch(\n",
    "    hosts=OPENSEARCH_HTTPS_URL,\n",
    "    http_auth=(OPENSEARCH_USER, OPENSEARCH_PASSWORD),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assety_hostname=False,\n",
    "    ssl_show_warn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(index, mappings=None, opensearch_client=client, replace_existing=False, number_of_shards=1):\n",
    "    \n",
    "    if replace_existing:\n",
    "        opensearch_client.indices.delete(index, ignore_unavailable=True)\n",
    "        print(f\"Deleted existing index: {index}\")\n",
    "\n",
    "    index_body = {\n",
    "        'settings': {\n",
    "            'index': {\n",
    "                'knn': True,\n",
    "                \"knn.algo_param.ef_search\": 256,\n",
    "                'number_of_shards':number_of_shards\n",
    "            }\n",
    "        },\n",
    "        \"mappings\" : mappings\n",
    "    }\n",
    "    response = opensearch_client.indices.create(index=index_name,body=index_body)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing index: my-multi-vector-index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'my-multi-vector-index'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappings = {\n",
    "    \"properties\" :  {\n",
    "        \"page_content_vector\" : {\n",
    "            \"type\" : \"knn_vector\",\n",
    "            \"dimension\": embeddings.client.get_sentence_embedding_dimension(),\n",
    "            \"method\": {\n",
    "            \"name\": \"hnsw\"\n",
    "          }\n",
    "        },\n",
    "        \"summary_vector\" : {\n",
    "            \"type\" : \"knn_vector\",\n",
    "            \"dimension\": embeddings.client.get_sentence_embedding_dimension(),\n",
    "            \"method\": {\n",
    "            \"name\": \"hnsw\"\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "create_index(index=index_name, mappings=mappings, replace_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_actions_string = \"\\n\".join([\n",
    "f\"\"\"{json.dumps({\"index\": {\n",
    "                \"_index\":index_name,\n",
    "                 \"_id\":sha1(doc[\"page_content\"].encode()).hexdigest()\n",
    "                 }})}\n",
    "{json.dumps(doc)}\n",
    "\"\"\" for doc in opensearch_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'took': 106,\n",
       " 'errors': False,\n",
       " 'items': [{'index': {'_index': 'my-multi-vector-index',\n",
       "    '_id': 'ab8352845164f737ca53ec4512fecdee74c2b8bf',\n",
       "    '_version': 1,\n",
       "    'result': 'created',\n",
       "    '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n",
       "    '_seq_no': 0,\n",
       "    '_primary_term': 1,\n",
       "    'status': 201}},\n",
       "  {'index': {'_index': 'my-multi-vector-index',\n",
       "    '_id': 'fd86971a87cc9f5715742fc51db79c48a902b144',\n",
       "    '_version': 1,\n",
       "    'result': 'created',\n",
       "    '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n",
       "    '_seq_no': 1,\n",
       "    '_primary_term': 1,\n",
       "    'status': 201}},\n",
       "  {'index': {'_index': 'my-multi-vector-index',\n",
       "    '_id': '4e989efd47a337b4e12ba7acbfe0a68902939441',\n",
       "    '_version': 1,\n",
       "    'result': 'created',\n",
       "    '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n",
       "    '_seq_no': 2,\n",
       "    '_primary_term': 1,\n",
       "    'status': 201}},\n",
       "  {'index': {'_index': 'my-multi-vector-index',\n",
       "    '_id': '4ca995a45bcdbea2499d687bfe71c00dc6a575c7',\n",
       "    '_version': 1,\n",
       "    'result': 'created',\n",
       "    '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n",
       "    '_seq_no': 3,\n",
       "    '_primary_term': 1,\n",
       "    'status': 201}},\n",
       "  {'index': {'_index': 'my-multi-vector-index',\n",
       "    '_id': '6f17a931e65e12aedc63d55e9046e5ec8eb80c8f',\n",
       "    '_version': 1,\n",
       "    'result': 'created',\n",
       "    '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n",
       "    '_seq_no': 4,\n",
       "    '_primary_term': 1,\n",
       "    'status': 201}},\n",
       "  {'index': {'_index': 'my-multi-vector-index',\n",
       "    '_id': '9d3e9edef3b537bbd36dd41964cefd822edffd4c',\n",
       "    '_version': 1,\n",
       "    'result': 'created',\n",
       "    '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n",
       "    '_seq_no': 5,\n",
       "    '_primary_term': 1,\n",
       "    'status': 201}},\n",
       "  {'index': {'_index': 'my-multi-vector-index',\n",
       "    '_id': 'ab219f6c14e3b8feedacfbab62f6a2c7414e7b32',\n",
       "    '_version': 1,\n",
       "    'result': 'created',\n",
       "    '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n",
       "    '_seq_no': 6,\n",
       "    '_primary_term': 1,\n",
       "    'status': 201}},\n",
       "  {'index': {'_index': 'my-multi-vector-index',\n",
       "    '_id': '85eb7919ef1a8c1a6e33a1c623bd3460cde6b1f0',\n",
       "    '_version': 1,\n",
       "    'result': 'created',\n",
       "    '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n",
       "    '_seq_no': 7,\n",
       "    '_primary_term': 1,\n",
       "    'status': 201}},\n",
       "  {'index': {'_index': 'my-multi-vector-index',\n",
       "    '_id': '68aac596d92602a84a01298da040055ce87507a8',\n",
       "    '_version': 1,\n",
       "    'result': 'created',\n",
       "    '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n",
       "    '_seq_no': 8,\n",
       "    '_primary_term': 1,\n",
       "    'status': 201}},\n",
       "  {'index': {'_index': 'my-multi-vector-index',\n",
       "    '_id': 'b1ed162bd4c6b33c39e2207c6733dd1afcfa262f',\n",
       "    '_version': 1,\n",
       "    'result': 'created',\n",
       "    '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n",
       "    '_seq_no': 9,\n",
       "    '_primary_term': 1,\n",
       "    'status': 201}}]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.bulk(bulk_actions_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_field_search(query:str, field_name:str, k=3, size=3, index=index_name, opensearch_client=client, embedding_model=embeddings):\n",
    "    \"\"\"k is the number of neighbors the search of each graph will return. You must also include the size option, which indicates how many results the query actually returns. \n",
    "    The plugin returns k amount of results for each shard (and each segment) and size amount of results for the entire query. The plugin supports a maximum k value of 10,000.\n",
    "    \"\"\"\n",
    "    query_body = {\n",
    "        \"size\": size,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                field_name : {\n",
    "                    \"vector\": generate_embeddings(text=query, embedding_model=embedding_model),\n",
    "                    \"k\": k\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # query_body = {\n",
    "    #     \"size\": size,\n",
    "    #     \"query\": {\n",
    "    #     \"script_score\": {\n",
    "    #         \"query\": {\n",
    "    #         \"match_all\": {}\n",
    "    #         },\n",
    "    #         \"script\": {\n",
    "    #         \"source\": \"knn_score\",\n",
    "    #         \"lang\": \"knn\",\n",
    "    #         \"params\": {\n",
    "    #             \"field\": field_name,\n",
    "    #             \"query_value\": generate_embeddings(text=query, embedding_model=embedding_model),\n",
    "    #             \"space_type\": \"cosinesimil\"\n",
    "    #         }\n",
    "    #         }\n",
    "    #     }\n",
    "    #     }\n",
    "    #     }\n",
    "\n",
    "    return opensearch_client.search(index=index, body=query_body)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_field_search(query=\"International supply chain\", field_name=\"page_content_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_content': 'I have a better plan to fight inflation. \\n\\nLower your costs, not your wages. \\n\\nMake more cars and semiconductors in America. \\n\\nMore infrastructure and innovation in America. \\n\\nMore goods moving faster and cheaper in America. \\n\\nMore jobs where you can earn a good living in America. \\n\\nAnd instead of relying on foreign supply chains, let’s make it in America. \\n\\nEconomists call it “increasing the productive capacity of our economy.” \\n\\nI call it building a better America. \\n\\nMy plan to fight inflation will lower your costs and lower the deficit. \\n\\n17 Nobel laureates in economics say my plan will ease long-term inflationary pressures. Top business leaders and most Americans support my plan. And here’s the plan: \\n\\nFirst – cut the cost of prescription drugs. Just look at insulin. One in ten Americans has diabetes. In Virginia, I met a 13-year-old boy named Joshua Davis.',\n",
       " 'summary': 'The author proposes a plan to combat inflation by increasing domestic production and reducing reliance on foreign supply chains. The plan involves investing in infrastructure, innovation, and job creation in America, while also cutting costs in areas such as prescription drug prices. The author cites support from 17 Nobel laureates in economics and top business leaders for their plan.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v for k,v in results[\"hits\"][\"hits\"][0][\"_source\"].items() if k in [\"page_content\",\"summary\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BELOW STILL IN PROGRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval with Reranking (Boosting with Text Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenSearch(\n",
    "    hosts=OPENSEARCH_HTTPS_URL,\n",
    "    http_auth=(OPENSEARCH_USER, OPENSEARCH_PASSWORD),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assety_hostname=False,\n",
    "    ssl_show_warn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_query = {\n",
    "  'size': 5,\n",
    "  'query': {\n",
    "    'multi_match': {\n",
    "      'query': query,\n",
    "    #   'fields': ['title^2', 'director']\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_res = client.search(os_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os_res[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
