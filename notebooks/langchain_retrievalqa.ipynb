{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline, AutoModelForCausalLM, AutoModelForQuestionAnswering, AutoModel\n",
    "from langchain import HuggingFacePipeline, HuggingFaceHub\n",
    "from langchain.document_loaders import UnstructuredURLLoader, UnstructuredPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import pickle\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RetrievalQA With Local HuggingFace Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01708ab0a6049cf8a15280c00f1e48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xl\", \n",
    "                                              min_length=10, \n",
    "                                            #   max_length=200\n",
    "                                              )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "pipe = pipeline(task=\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\")\n",
    "\n",
    "# llm = HuggingFacePipeline.from_model_id(\"google/flan-t5-xl\", task=\"text2text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://python.langchain.com/docs/get_started/introduction\"]\n",
    "loader = UnstructuredURLLoader(urls = urls)\n",
    "documents = loader.load()\n",
    "\n",
    "# pdf_paths = glob(\"path/to/pdfs/*\")\n",
    "# documents = []\n",
    "# for path in pdf_paths:\n",
    "#     loader = UnstructuredPDFLoader(file_path=path)\n",
    "#     documents.extend(loader.load())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=75\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Get started\\n\\nIntroduction\\n\\nIntroduction\\n\\nLangChain is a framework for developing applications powered by language models. It enables applications that are:\\n\\nData-aware: connect a language model to other sources of data\\n\\nAgentic: allow a language model to interact with its environment', metadata={'source': 'https://python.langchain.com/docs/get_started/introduction'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "##Load Embedding Model to Create Vectors from Documents\n",
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-large\",\n",
    "    model_kwargs={\"device\":\"cpu\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line can take a while to run. Will save vector store in next cell so it can be loaded in from disk in subsequent runs\n",
    "vector_store = FAISS.from_documents(texts,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vector_store.db\", \"wb\") as f:\n",
    "    pickle.dump(vector_store, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vector_store.db\", \"rb\") as f:\n",
    "    vector_store = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vector_store.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modify this query to be relevant to the documents stored in the vector store\n",
    "query = \"What is Langchain?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Get started\\n\\nIntroduction\\n\\nIntroduction\\n\\nLangChain is a framework for developing applications powered by language models. It enables applications that are:\\n\\nData-aware: connect a language model to other sources of data\\n\\nAgentic: allow a language model to interact with its environment', metadata={'source': 'https://python.langchain.com/docs/get_started/introduction'}),\n",
       " Document(page_content='Guides\\u200b\\n\\nLearn best practices for developing with LangChain.\\n\\nEcosystem\\u200b\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations and dependent repos.\\n\\nAdditional resources\\u200b', metadata={'source': 'https://python.langchain.com/docs/get_started/introduction'}),\n",
       " Document(page_content='Community\\u200b\\n\\nHead to the Community navigator to find places to ask questions, share feedback, meet other developers, and dream about the future of LLMâ€™s.\\n\\nAPI reference\\u200b\\n\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python package.', metadata={'source': 'https://python.langchain.com/docs/get_started/introduction'}),\n",
       " Document(page_content='Note: These docs are for the LangChain Python package. For documentation on LangChain.js, the JS/TS version, head here.\\n\\nModules\\u200b\\n\\nLangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:\\n\\nModel I/O\\u200b', metadata={'source': 'https://python.langchain.com/docs/get_started/introduction'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.search(query, \"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/github/generative-ai/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a framework for developing applications powered by language models'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## These results include the knowledge base\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a chain of arithmetical operations'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## These results do not include the knowledge base (just raw input/output with model)\n",
    "llm.predict(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/github/generative-ai/venv/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:179: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "Some weights of the model checkpoint at facebook/rag-token-nq were not used when initializing RagTokenForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.weight', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RagTokenForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RagTokenForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True)\n",
    "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " michael phelps\n"
     ]
    }
   ],
   "source": [
    "input_dict = tokenizer.prepare_seq2seq_batch(\"who holds the record in 100m freestyle\", return_tensors=\"pt\") \n",
    "\n",
    "generated = model.generate(input_ids=input_dict[\"input_ids\"]) \n",
    "print(tokenizer.batch_decode(generated, skip_special_tokens=True)[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
