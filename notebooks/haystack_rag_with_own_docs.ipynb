{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MUST BE RUNNING AN ELASTICSEARCH INSTANCE BEFORE RUNNING NOTEBOOK\n",
    "# docker command - `docker run --name elastic -p 9200:9200 -e \"discovery.type=single-node\" -m 1G -itd docker.elastic.co/elasticsearch/elasticsearch:7.9.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "from haystack.utils import fetch_archive_from_http\n",
    "from haystack import Pipeline\n",
    "from haystack.nodes import TextConverter, PreProcessor, BM25Retriever, FARMReader, PDFToTextConverter\n",
    "\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "log = logging.getLogger(\"haystack\")\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the host where Elasticsearch is running, default to localhost\n",
    "host = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\n",
    "\n",
    "document_store = ElasticsearchDocumentStore(host=host, username=\"\", password=\"\", index=\"document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = \"../data/pdfs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "pdf_converter = PDFToTextConverter()\n",
    "preprocessor = PreProcessor(\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=True,\n",
    "    clean_empty_lines=True,\n",
    "    split_by=\"word\",\n",
    "    split_length=200,\n",
    "    split_overlap=20,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_pipeline.add_node(component=pdf_converter, name=\"TextConverter\", inputs=[\"File\"])\n",
    "indexing_pipeline.add_node(component=preprocessor, name=\"PreProcessor\", inputs=[\"TextConverter\"])\n",
    "indexing_pipeline.add_node(component=document_store, name=\"DocumentStore\", inputs=[\"PreProcessor\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_index = [doc_dir + \"/\" + f for f in os.listdir(doc_dir)]\n",
    "indexing_pipeline.run_batch(file_paths=files_to_index)\n",
    "#As an alternative, you can cast you text data into Document objects and write them into the DocumentStore using DocumentStore.write_documents()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "querying_pipeline = Pipeline()\n",
    "querying_pipeline.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "querying_pipeline.add_node(component=reader, name=\"Reader\", inputs=[\"Retriever\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = querying_pipeline.run(\n",
    "    query=\"Can I service my air-conditioning myself?\", params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from haystack.utils import print_answers\n",
    "\n",
    "# print_answers(prediction, details=\"minimum\")  ## Choose from `minimum`, `medium` and `all`\n",
    "\n",
    "pprint(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Embeddings Instead of Text Document Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.utils import print_answers\n",
    "from haystack.nodes import EmbeddingRetriever\n",
    "from haystack.pipelines import ExtractiveQAPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAISS_INDEX_PATH = \"./faiss_index_pdf.faiss\"\n",
    "if os.path.exists(FAISS_INDEX_PATH) and os.path.exists(FAISS_INDEX_PATH.replace(\".faiss\",\".json\")):\n",
    "    log.warn(f\"Reading FAISS Index from {FAISS_INDEX_PATH}\")\n",
    "    document_store = FAISSDocumentStore.load(FAISS_INDEX_PATH)\n",
    "    retriever = EmbeddingRetriever(\n",
    "        document_store=document_store, embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "        )\n",
    "else:\n",
    "    log.warn(f\"Creating new FAISS Index\")\n",
    "    document_store = FAISSDocumentStore(sql_url=\"sqlite:///faiss_document_store_pdf.db\", faiss_index_factory_str=\"Flat\")\n",
    "    # Let's first get some files that we want to use\n",
    "\n",
    "    indexing_pipeline = Pipeline()\n",
    "    pdf_converter = PDFToTextConverter()\n",
    "    preprocessor = PreProcessor(\n",
    "        clean_whitespace=True,\n",
    "        clean_header_footer=True,\n",
    "        clean_empty_lines=True,\n",
    "        split_by=\"word\",\n",
    "        split_length=200,\n",
    "        split_overlap=20,\n",
    "        split_respect_sentence_boundary=True,\n",
    "    )\n",
    "    indexing_pipeline.add_node(component=pdf_converter, name=\"TextConverter\", inputs=[\"File\"])\n",
    "    indexing_pipeline.add_node(component=preprocessor, name=\"PreProcessor\", inputs=[\"TextConverter\"])\n",
    "    indexing_pipeline.add_node(component=document_store, name=\"DocumentStore\", inputs=[\"PreProcessor\"])\n",
    "    files_to_index = [doc_dir + \"/\" + f for f in os.listdir(doc_dir)]\n",
    "    indexing_pipeline.run_batch(file_paths=files_to_index)\n",
    "\n",
    "    retriever = EmbeddingRetriever(\n",
    "        document_store=document_store, embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "        )\n",
    "    # Important:\n",
    "    # Now that we initialized the Retriever, we need to call update_embeddings() to iterate over all\n",
    "    # previously indexed documents and update their embedding representation.\n",
    "    # While this can be a time consuming operation (depending on the corpus size), it only needs to be done once.\n",
    "    # At query time, we only need to embed the query and compare it to the existing document embeddings, which is very fast.\n",
    "    document_store.update_embeddings(retriever)\n",
    "\n",
    "    document_store.save(FAISS_INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipe.run(\n",
    "    query=\"What brand of motor oil should I use?\", params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_answers(prediction, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Agent to Use the QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.agents import Agent, Tool\n",
    "from haystack.nodes import PromptNode\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from torch import float16 as torchfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torchfloat16,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "# MODEL_ID = \"tiiuae/falcon-7b-instruct\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_ID,\n",
    "#     trust_remote_code=True,\n",
    "#     quantization_config=quantization_config\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# prompt_node = PromptNode(MODEL_ID, model_kwargs={\"model\":model, \"tokenizer\": tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_API_KEY = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "MODEL_ID = \"google/flan-t5-xxl\"\n",
    "# MODEL_ID = \"tiiuae/falcon-7b\"\n",
    "prompt_node = PromptNode(model_name_or_path=MODEL_ID, stop_words=[\"Observation:\"], api_key=HUGGINGFACE_API_KEY)\n",
    "\n",
    "agent = Agent(prompt_node=prompt_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = Tool(\n",
    "    name=\"F150_Car_Truck_QA\",\n",
    "    pipeline_or_node=pipe,\n",
    "    description=\"useful for when you need to answer questions related to vehicles, trucks, F150\",\n",
    "    output_variable=\"answers\",\n",
    ")\n",
    "agent.add_tool(search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.run(\"What PSI should I inflate my tires to?\")\n",
    "\n",
    "print(result[\"transcript\"].split(\"---\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEXT - https://haystack.deepset.ai/tutorials/02_finetune_a_model_on_your_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
