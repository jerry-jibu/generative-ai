{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MUST BE RUNNING AN ELASTICSEARCH INSTANCE BEFORE RUNNING NOTEBOOK\n",
    "# docker command - `docker run --name elastic -p 9200:9200 -e \"discovery.type=single-node\" -m 1G -itd docker.elastic.co/elasticsearch/elasticsearch:7.9.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "from haystack.utils import fetch_archive_from_http\n",
    "from haystack import Pipeline\n",
    "from haystack.nodes import TextConverter, PreProcessor, BM25Retriever, FARMReader, PDFToTextConverter\n",
    "\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "log = logging.getLogger(\"haystack\")\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the host where Elasticsearch is running, default to localhost\n",
    "host = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\n",
    "\n",
    "document_store = ElasticsearchDocumentStore(host=host, username=\"\", password=\"\", index=\"document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = \"../data/pdfs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "pdf_converter = PDFToTextConverter()\n",
    "preprocessor = PreProcessor(\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=True,\n",
    "    clean_empty_lines=True,\n",
    "    split_by=\"word\",\n",
    "    split_length=200,\n",
    "    split_overlap=20,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_pipeline.add_node(component=pdf_converter, name=\"TextConverter\", inputs=[\"File\"])\n",
    "indexing_pipeline.add_node(component=preprocessor, name=\"PreProcessor\", inputs=[\"TextConverter\"])\n",
    "indexing_pipeline.add_node(component=document_store, name=\"DocumentStore\", inputs=[\"PreProcessor\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_index = [doc_dir + \"/\" + f for f in os.listdir(doc_dir)]\n",
    "indexing_pipeline.run_batch(file_paths=files_to_index)\n",
    "#As an alternative, you can cast you text data into Document objects and write them into the DocumentStore using DocumentStore.write_documents()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "querying_pipeline = Pipeline()\n",
    "querying_pipeline.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "querying_pipeline.add_node(component=reader, name=\"Reader\", inputs=[\"Retriever\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = querying_pipeline.run(\n",
    "    query=\"Can I service my air-conditioning myself?\", params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from haystack.utils import print_answers\n",
    "\n",
    "# print_answers(prediction, details=\"minimum\")  ## Choose from `minimum`, `medium` and `all`\n",
    "\n",
    "pprint(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Embeddings Instead of Text Document Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.utils import print_answers\n",
    "from haystack.nodes import EmbeddingRetriever\n",
    "from haystack.pipelines import ExtractiveQAPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6568/31301763.py:3: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  log.warn(f\"Reading FAISS Index from {FAISS_INDEX_PATH}\")\n",
      "WARNING - haystack -  Reading FAISS Index from ./faiss_index_pdf.faiss\n",
      "INFO - haystack.modeling.utils -  Using devices: CPU - Number of GPUs: 0\n",
      "INFO - haystack.nodes.retriever.dense -  Init retriever using embeddings of model sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
      "/home/ryan/github/generative-ai/venv/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "FAISS_INDEX_PATH = \"./faiss_index_pdf.faiss\"\n",
    "if os.path.exists(FAISS_INDEX_PATH) and os.path.exists(FAISS_INDEX_PATH.replace(\".faiss\",\".json\")):\n",
    "    log.warn(f\"Reading FAISS Index from {FAISS_INDEX_PATH}\")\n",
    "    document_store = FAISSDocumentStore.load(FAISS_INDEX_PATH)\n",
    "    retriever = EmbeddingRetriever(\n",
    "        document_store=document_store, embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "        )\n",
    "else:\n",
    "    log.warn(f\"Creating new FAISS Index\")\n",
    "    document_store = FAISSDocumentStore(sql_url=\"sqlite:///faiss_document_store_pdf.db\", faiss_index_factory_str=\"Flat\")\n",
    "    # Let's first get some files that we want to use\n",
    "\n",
    "    indexing_pipeline = Pipeline()\n",
    "    pdf_converter = PDFToTextConverter()\n",
    "    preprocessor = PreProcessor(\n",
    "        clean_whitespace=True,\n",
    "        clean_header_footer=True,\n",
    "        clean_empty_lines=True,\n",
    "        split_by=\"word\",\n",
    "        split_length=200,\n",
    "        split_overlap=20,\n",
    "        split_respect_sentence_boundary=True,\n",
    "    )\n",
    "    indexing_pipeline.add_node(component=pdf_converter, name=\"TextConverter\", inputs=[\"File\"])\n",
    "    indexing_pipeline.add_node(component=preprocessor, name=\"PreProcessor\", inputs=[\"TextConverter\"])\n",
    "    indexing_pipeline.add_node(component=document_store, name=\"DocumentStore\", inputs=[\"PreProcessor\"])\n",
    "    files_to_index = [doc_dir + \"/\" + f for f in os.listdir(doc_dir)]\n",
    "    indexing_pipeline.run_batch(file_paths=files_to_index)\n",
    "\n",
    "    retriever = EmbeddingRetriever(\n",
    "        document_store=document_store, embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "        )\n",
    "    # Important:\n",
    "    # Now that we initialized the Retriever, we need to call update_embeddings() to iterate over all\n",
    "    # previously indexed documents and update their embedding representation.\n",
    "    # While this can be a time consuming operation (depending on the corpus size), it only needs to be done once.\n",
    "    # At query time, we only need to embed the query and compare it to the existing document embeddings, which is very fast.\n",
    "    document_store.update_embeddings(retriever)\n",
    "\n",
    "    document_store.save(FAISS_INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CPU - Number of GPUs: 0\n",
      "INFO - haystack.modeling.utils -  Using devices: CPU - Number of GPUs: 0\n",
      "INFO - haystack.modeling.model.language_model -   * LOADING MODEL: 'deepset/roberta-base-squad2' (Roberta)\n",
      "INFO - haystack.modeling.model.language_model -  Auto-detected model language: english\n",
      "INFO - haystack.modeling.model.language_model -  Loaded 'deepset/roberta-base-squad2' (Roberta model) from model hub.\n",
      "INFO - haystack.modeling.utils -  Using devices: CPU - Number of GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipe.run(\n",
    "    query=\"What brand of motor oil should I use?\", params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_answers(prediction, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Agent to Use the QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.agents import Agent, Tool\n",
    "from haystack.nodes import PromptNode\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from torch import float16 as torchfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torchfloat16,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "# MODEL_ID = \"EleutherAI/pythia-1b\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_ID,\n",
    "#     trust_remote_code=True,\n",
    "#     # quantization_config=quantization_config\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# prompt_node = PromptNode(MODEL_ID, model_kwargs={\"model\":model, \"tokenizer\": tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85be98428064bcb8823e08831da04b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5b498ade5a48bc9e7a3d0ef21b0737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c161605ecb4ceea212d2605a9910b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c638a67ecab4dd2b2fc9dc590a77626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HUGGINGFACE_API_KEY = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "MODEL_ID = \"google/flan-t5-xxl\"\n",
    "# MODEL_ID = \"tiiuae/falcon-7b\"\n",
    "# MODEL_ID = \"EleutherAI/pythia-1b\"\n",
    "# MODEL_ID = \"PY007/TinyLlama-1.1B-step-50K-105b\"\n",
    "prompt_node = PromptNode(model_name_or_path=MODEL_ID, stop_words=[\"Observation:\"], api_key=HUGGINGFACE_API_KEY)\n",
    "\n",
    "agent = Agent(prompt_node=prompt_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = Tool(\n",
    "    name=\"F150_Car_Truck_QA\",\n",
    "    pipeline_or_node=pipe,\n",
    "    description=\"useful for when you need to answer questions related to vehicles, trucks, F150\",\n",
    "    output_variable=\"answers\",\n",
    ")\n",
    "agent.add_tool(search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent zero-shot-react started with {'query': 'What PSI should I inflate my tires to?', 'params': None}\n"
     ]
    },
    {
     "ename": "HuggingFaceInferenceError",
     "evalue": "HuggingFace Inference returned an error.\nStatus code: 503\nResponse body: {\"error\":\"Model google/flan-t5-xxl is currently loading\",\"estimated_time\":1802.7086181640625}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/github/generative-ai/venv/lib/python3.10/site-packages/haystack/nodes/prompt/invocation_layer/hugging_face_inference.py:237\u001b[0m, in \u001b[0;36mHFInferenceEndpointInvocationLayer._post\u001b[0;34m(self, data, stream, attempts, status_codes_to_retry, timeout)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     response \u001b[39m=\u001b[39m request_with_retry(\n\u001b[1;32m    238\u001b[0m         method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    239\u001b[0m         status_codes_to_retry\u001b[39m=\u001b[39;49mstatus_codes_to_retry,\n\u001b[1;32m    240\u001b[0m         attempts\u001b[39m=\u001b[39;49mattempts,\n\u001b[1;32m    241\u001b[0m         url\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl,\n\u001b[1;32m    242\u001b[0m         headers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    243\u001b[0m         json\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    244\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    245\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    246\u001b[0m     )\n\u001b[1;32m    247\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mHTTPError \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/github/generative-ai/venv/lib/python3.10/site-packages/haystack/utils/requests_utils.py:93\u001b[0m, in \u001b[0;36mrequest_with_retry\u001b[0;34m(attempts, status_codes_to_retry, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39m# We raise here too in case the request failed with a status code that\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m# won't trigger a retry, this way the call will still cause an explicit exception\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m res\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m     94\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/github/generative-ai/venv/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 503 Server Error: Service Unavailable for url: https://api-inference.huggingface.co/models/google/flan-t5-xxl",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHuggingFaceInferenceError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mWhat PSI should I inflate my tires to?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(result[\u001b[39m\"\u001b[39m\u001b[39mtranscript\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m---\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/github/generative-ai/venv/lib/python3.10/site-packages/haystack/agents/base.py:359\u001b[0m, in \u001b[0;36mAgent.run\u001b[0;34m(self, query, max_steps, params)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m agent_step\u001b[39m.\u001b[39mis_last():\n\u001b[0;32m--> 359\u001b[0m         agent_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step(query, agent_step, params)\n\u001b[1;32m    360\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_agent_finish(agent_step)\n",
      "File \u001b[0;32m~/github/generative-ai/venv/lib/python3.10/site-packages/haystack/agents/base.py:366\u001b[0m, in \u001b[0;36mAgent._step\u001b[0;34m(self, query, current_step, params)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_step\u001b[39m(\u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, current_step: AgentStep, params: Optional[\u001b[39mdict\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    365\u001b[0m     \u001b[39m# plan next step using the LLM\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m     prompt_node_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plan(query, current_step)\n\u001b[1;32m    368\u001b[0m     \u001b[39m# from the LLM response, create the next step\u001b[39;00m\n\u001b[1;32m    369\u001b[0m     next_step \u001b[39m=\u001b[39m current_step\u001b[39m.\u001b[39mcreate_next_step(prompt_node_response)\n",
      "File \u001b[0;32m~/github/generative-ai/venv/lib/python3.10/site-packages/haystack/agents/base.py:391\u001b[0m, in \u001b[0;36mAgent._plan\u001b[0;34m(self, query, current_step)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_prompt_template(template_params)\n\u001b[1;32m    390\u001b[0m \u001b[39m# invoke via prompt node\u001b[39;00m\n\u001b[0;32m--> 391\u001b[0m prompt_node_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprompt_node\u001b[39m.\u001b[39;49mprompt(\n\u001b[1;32m    392\u001b[0m     prompt_template\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprompt_template,\n\u001b[1;32m    393\u001b[0m     stream_handler\u001b[39m=\u001b[39;49mAgentTokenStreamingHandler(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallback_manager),\n\u001b[1;32m    394\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtemplate_params,\n\u001b[1;32m    395\u001b[0m )\n\u001b[1;32m    396\u001b[0m \u001b[39mreturn\u001b[39;00m prompt_node_response\n",
      "File \u001b[0;32m~/github/generative-ai/venv/lib/python3.10/site-packages/haystack/nodes/prompt/prompt_node.py:167\u001b[0m, in \u001b[0;36mPromptNode.prompt\u001b[0;34m(self, prompt_template, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m     prompt_collector\u001b[39m.\u001b[39mappend(prompt)\n\u001b[1;32m    166\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mPrompt being sent to LLM with prompt \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and kwargs \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, prompt, kwargs_copy)\n\u001b[0;32m--> 167\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprompt_model\u001b[39m.\u001b[39;49minvoke(prompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_copy)\n\u001b[1;32m    168\u001b[0m     results\u001b[39m.\u001b[39mextend(output)\n\u001b[1;32m    170\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mprompts\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m prompt_collector\n",
      "File \u001b[0;32m~/github/generative-ai/venv/lib/python3.10/site-packages/haystack/nodes/prompt/prompt_model.py:111\u001b[0m, in \u001b[0;36mPromptModel.invoke\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, prompt: Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m], List[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m    104\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    Takes in a prompt and returns a list of responses using the underlying invocation layer.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39m    :return: A list of model-generated responses for the prompt or prompts.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_invocation_layer\u001b[39m.\u001b[39;49minvoke(prompt\u001b[39m=\u001b[39;49mprompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/github/generative-ai/venv/lib/python3.10/site-packages/haystack/nodes/prompt/invocation_layer/hugging_face_inference.py:173\u001b[0m, in \u001b[0;36mHFInferenceEndpointInvocationLayer.invoke\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39m# see https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task\u001b[39;00m\n\u001b[1;32m    155\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[1;32m    156\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbest_of\u001b[39m\u001b[39m\"\u001b[39m: kwargs_with_defaults\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbest_of\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    157\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdetails\u001b[39m\u001b[39m\"\u001b[39m: kwargs_with_defaults\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdetails\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwatermark\u001b[39m\u001b[39m\"\u001b[39m: kwargs_with_defaults\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mwatermark\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m    172\u001b[0m }\n\u001b[0;32m--> 173\u001b[0m response: requests\u001b[39m.\u001b[39mResponse \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    174\u001b[0m     data\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39minputs\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt, \u001b[39m\"\u001b[39;49m\u001b[39mparameters\u001b[39;49m\u001b[39m\"\u001b[39;49m: params, \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream}, stream\u001b[39m=\u001b[39;49mstream\n\u001b[1;32m    175\u001b[0m )\n\u001b[1;32m    176\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    177\u001b[0m     handler: TokenStreamingHandler \u001b[39m=\u001b[39m kwargs_with_defaults\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mstream_handler\u001b[39m\u001b[39m\"\u001b[39m, DefaultTokenStreamingHandler())\n",
      "File \u001b[0;32m~/github/generative-ai/venv/lib/python3.10/site-packages/haystack/nodes/prompt/invocation_layer/hugging_face_inference.py:254\u001b[0m, in \u001b[0;36mHFInferenceEndpointInvocationLayer._post\u001b[0;34m(self, data, stream, attempts, status_codes_to_retry, timeout)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[39mif\u001b[39;00m res\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m401\u001b[39m:\n\u001b[1;32m    252\u001b[0m         \u001b[39mraise\u001b[39;00m HuggingFaceInferenceUnauthorizedError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAPI key is invalid: \u001b[39m\u001b[39m{\u001b[39;00mres\u001b[39m.\u001b[39mtext\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 254\u001b[0m     \u001b[39mraise\u001b[39;00m HuggingFaceInferenceError(\n\u001b[1;32m    255\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHuggingFace Inference returned an error.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mStatus code: \u001b[39m\u001b[39m{\u001b[39;00mres\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mResponse body: \u001b[39m\u001b[39m{\u001b[39;00mres\u001b[39m.\u001b[39mtext\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    256\u001b[0m         status_code\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mstatus_code,\n\u001b[1;32m    257\u001b[0m     )\n\u001b[1;32m    258\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "\u001b[0;31mHuggingFaceInferenceError\u001b[0m: HuggingFace Inference returned an error.\nStatus code: 503\nResponse body: {\"error\":\"Model google/flan-t5-xxl is currently loading\",\"estimated_time\":1802.7086181640625}"
     ]
    }
   ],
   "source": [
    "result = agent.run(\"What PSI should I inflate my tires to?\")\n",
    "\n",
    "print(result[\"transcript\"].split(\"---\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEXT - https://haystack.deepset.ai/tutorials/02_finetune_a_model_on_your_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
