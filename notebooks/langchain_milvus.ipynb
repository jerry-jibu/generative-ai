{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENSURE THAT THE ETCD, MINIO, AND MILVUS SERVICES IN docker-compose.yml\n",
    "### ARE UNCOMMENTED AND RUNNING IF YOU WANT TO USE THE CONTAINERIZED INSTANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Milvus\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from textwrap import dedent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MILVUS_HOST = \"localhost\" # When running notebook outside of compoose jupyter container\n",
    "MILVUS_HOST = \"milvus\" # When running notebook inside of compose jupyter container\n",
    "MILVUS_PORT = 19530\n",
    "# MILVUS_HTTPS_URL= f\"https://{MILVUS_HOST}:{MILVUS_PORT}\"\n",
    "MILVUS_USER = \"root\"\n",
    "MILVUS_PASSWORD = \"milvus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model_name = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = TextLoader(\"../data/state_of_the_union.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=75)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Milvus.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    connection_args={\"host\":MILVUS_HOST, \"port\":MILVUS_PORT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much does the president want to cut the cancer death rate?\"\n",
    "docs = docsearch.similarity_search(query, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cancer is the #2 cause of death in Americaâ€“second only to heart disease. \\n\\nLast month, I announced our plan to supercharge  \\nthe Cancer Moonshot that President Obama asked me to lead six years ago. \\n\\nOur goal is to cut the cancer death rate by at least 50% over the next 25 years, turn more cancers from death sentences into treatable diseases.  \\n\\nMore support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTANT NOTE If you want to use the Mistral 7B Model\n",
    "## As of 10/7/2023, need to run the pip install below, as Mistral is not included in main transformers library yet\n",
    "# !pip install git+https://github.com/huggingface/transformers.git\n",
    "\n",
    "## RESTART THE KERNEL AFTER INSTALLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n",
    "# model_name = \"TheBloke/CollectiveCognition-v1.1-Mistral-7B-GPTQ\"\n",
    "model_name = \"TheBloke/Llama-2-7B-chat-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             device_map=\"auto\",\n",
    "                                             )\n",
    "# model=model.to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          device_map=\"auto\"\n",
    "                                          )\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model without any additional context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much does the president want to cut the cancer death rate?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(query, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_tensor = model.generate(inputs, min_new_tokens=25, max_new_tokens=100, repetition_penalty=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,  1128,  1568,   947,   278,  6673,   864,   304,  5700,   278,\n",
       "        23900,  4892,  6554, 29973,    13,    13,  1576,  7178, 29915, 29879,\n",
       "         1815,  2265,  4546,   787,  8711, 14511,  1230,   263,  9893,   304,\n",
       "        10032,   278,  1353,   310, 23900,  4892, 29879,   491, 29871, 29945,\n",
       "        29900, 29995,   975,   278,  2446,   316,  6332, 29889,   910,   626,\n",
       "         2966,  2738,  7306,   674,  1996,  7282, 13258,  1860,   297,  5925,\n",
       "        29892,   848, 19383, 29892,   322,   716,  7539,  1860, 29892,   408,\n",
       "         1532,   408,  3620,   297,  9045, 18020, 24833,   322, 23274, 29889,\n",
       "          450,  8037,  5619,   756,  7972,   395, 29896, 24464,   297,  5220,\n",
       "          292,   363,   278,  1824, 29892,   607,   338,  3806,   304,   367,\n",
       "        19228,   491,  2024,  1016,   943,   322,   916,  8974,   310, 29199,\n",
       "        29889,     2], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the token ID's for the generated response\n",
    "output_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decode the tokens like this (instead of decoding the entire output_tensor)\n",
    "## so that we don't include the propmt itself in the output\n",
    "generated_text = tokenizer.batch_decode(output_tensor[:, inputs.shape[1]:])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The President's Cancer Moonshot initiative aims to reduce the number of cancer deaths by 50% over the next decade. This ambitious goal will require significant investments in research, data sharing, and new treatments, as well as changes in healthcare policies and practices. The White House has proposed $1 billion in funding for the program, which is expected to be matched by private donors and other sources of funds.</s>\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model with retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_retrieval_qa(query, k=4, min_new_tokens=1, max_new_tokens=20, return_sources=True, repetition_penalty=1.0, remove_tokens=(\"<s>\",\"</s>\")):\n",
    "    docs = docsearch.similarity_search(query, k=k)\n",
    "\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    template = PromptTemplate(template=dedent(f\"\"\"\"\n",
    "    <SYS>You are a question-and-answer assistant that only uses information from the provided context when responding.</SYS>\n",
    "    \n",
    "    <s>[INST] Use the context to answer the question. Be detailed and specific in your answers, but do not include anything besides the answer to the question in your response.\n",
    "    \n",
    "    Context: {context}\n",
    "\n",
    "    Question: {{query}}[/INST]\n",
    "\n",
    "    Answer: \n",
    "    \"\"\"), input_variables=[\"query\"])\n",
    "\n",
    "    prompt = template.format(query=query)\n",
    "\n",
    "    input_tensor = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output_tensor = model.generate(input_tensor, min_new_tokens=min_new_tokens, max_new_tokens=max_new_tokens, repetition_penalty=repetition_penalty)\n",
    "\n",
    "    ## The batch_decode call below removes the input tokens\n",
    "    generated_text = tokenizer.batch_decode(output_tensor[:, input_tensor.shape[1]:])[0]\n",
    "    \n",
    "    for token in remove_tokens:\n",
    "        generated_text = generated_text.replace(token,\"\")\n",
    "    generated_text = generated_text.strip('\" \\n')\n",
    "    \n",
    "    output = {\n",
    "        \"text\": generated_text\n",
    "    }\n",
    "    if return_sources: output[\"sources\"] = docs\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_retrieval_qa(query, min_new_tokens=150, max_new_tokens=300, repetition_penalty=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The president wants to cut the cancer death rate by at least 50% over the next 25 years. This means reducing the number of cancer deaths by half within the next quarter-century. The president's plan is to achieve this through increased funding for research, improved access to healthcare, and greater support for patients and families affected by cancer. Specifically, the president has proposed increasing funding for the Advanced Research Projects Agency for Health (ARPA-H) to accelerate the development of new cancer treatments and diagnostic tools. Additionally, the president has called on Congress to provide greater support for patients and families affected by cancer, including those who are struggling with medical bills and other financial burdens. By taking these steps, the president hopes to make significant progress towards his goal of cutting the cancer death rate in half over the next 25 years.\n"
     ]
    }
   ],
   "source": [
    "print(res[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Multiple Vector Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a \"summary\" field for the documents we want to index, so we will have both a \"page_content\" field and a \"summary\" field to embed and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "milvus_docs = [doc.dict() for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text:str, min_new_tokens=1, max_new_tokens=200, repetition_penalty=1.1):\n",
    "    \n",
    "    summarization_template_string = \"\"\"[SYS]You are an assistant with a great ability for summarizing text content.[/SYS]\n",
    "    \n",
    "    <s>[INST]Summarize the following information. Capture the important information, but be as concise as possible.\n",
    "\n",
    "    Information: {document}[/INST]\n",
    "\n",
    "    Summary: \"\"\"\n",
    "    summarization_template = PromptTemplate(template=summarization_template_string, input_variables=[\"document\"])\n",
    "    \n",
    "    summarization_prompt = summarization_template.format(document=text)\n",
    "\n",
    "    input_tensor = tokenizer.encode(summarization_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output_tensor = model.generate(\n",
    "                                   input_tensor, \n",
    "                                   min_new_tokens=min_new_tokens, \n",
    "                                   max_new_tokens=max_new_tokens, \n",
    "                                   repetition_penalty=repetition_penalty\n",
    "                                  )\n",
    "\n",
    "    generated_text = tokenizer.batch_decode(output_tensor[:, input_tensor.shape[1]:])[0]\n",
    "    \n",
    "\n",
    "    try:\n",
    "        generated_text = generated_text.split(\"Summary: \")[1]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    replace_tokens = (\"<s>\",\"</s>\")\n",
    "    for token in replace_tokens:\n",
    "        generated_text = generated_text.replace(token,\"\")\n",
    "    generated_text = generated_text.strip()\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(milvus_docs):\n",
    "    doc[\"id\"] = i+1\n",
    "    text = doc[\"page_content\"]\n",
    "    summary = summarize_text(text)\n",
    "    doc[\"summary\"] = summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Milvus DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, db, MilvusException, utility\n",
    "\n",
    "conn = connections.connect(host=MILVUS_HOST, port=MILVUS_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPC error: [create_database], <MilvusException: (code=1, message=database already exist: mydb)>, <Time:{'RPC start': '2023-10-13 02:48:20.474212', 'RPC error': '2023-10-13 02:48:20.475708'}>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database mydb already exists. Skipping creation...\n"
     ]
    }
   ],
   "source": [
    "db_name = \"mydb\"\n",
    "try:\n",
    "    database = db.create_database(db_name)\n",
    "except MilvusException as e:\n",
    "    if e.code == 1:\n",
    "        print(f\"Database {db_name} already exists. Skipping creation...\")\n",
    "        db.using_database(db_name)\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['default', 'mydb']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.list_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Schema and Collection using the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import CollectionSchema, FieldSchema, DataType, Collection\n",
    "doc_id = FieldSchema(\n",
    "  name=\"id\",\n",
    "  dtype=DataType.INT64,\n",
    "  is_primary=True,\n",
    ")\n",
    "page_content = FieldSchema(\n",
    "  name=\"page_content\",\n",
    "  dtype=DataType.VARCHAR,\n",
    "  max_length=1000,\n",
    "  # The default value will be used if this field is left empty during data inserts or upserts.\n",
    "  # The data type of `default_value` must be the same as that specified in `dtype`.\n",
    "  default_value=\"NONE\"\n",
    ")\n",
    "page_content_vector = FieldSchema(\n",
    "  name=\"page_content_vector\",\n",
    "  dtype=DataType.FLOAT_VECTOR,\n",
    "  dim=embeddings.client.get_sentence_embedding_dimension()\n",
    ")\n",
    "summary = FieldSchema(\n",
    "  name=\"summary\",\n",
    "  dtype=DataType.VARCHAR,\n",
    "  max_length=1000,\n",
    "  # The default value will be used if this field is left empty during data inserts or upserts.\n",
    "  # The data type of `default_value` must be the same as that specified in `dtype`.\n",
    "  default_value=\"NONE\"\n",
    ")\n",
    "summary_vector = FieldSchema(\n",
    "  name=\"summary_vector\",\n",
    "  dtype=DataType.FLOAT_VECTOR,\n",
    "  dim=embeddings.client.get_sentence_embedding_dimension()\n",
    ")\n",
    "metadata = FieldSchema(\n",
    "    name=\"metadata\",\n",
    "    dtype=DataType.JSON\n",
    ")\n",
    "\n",
    "## HAVE TO CREATE MULTIPLE COLLECTIONS BECAUSE MILVUS MULTI-VECTOR NOT SUPPORTED YET (as of 10/12/2023)\n",
    "page_content_schema = CollectionSchema(\n",
    "  fields=[doc_id, page_content, page_content_vector, summary, metadata],\n",
    "  description=\"SOTU Search with page content vector\",\n",
    "  enable_dynamic_field=True\n",
    ")\n",
    "summary_schema = CollectionSchema(\n",
    "  fields=[doc_id, page_content, summary, summary_vector, metadata],\n",
    "  description=\"SOTU Search with summary vector\",\n",
    "  enable_dynamic_field=True\n",
    ")\n",
    "page_content_collection_name = \"sotu_text\"\n",
    "summary_collection_name = \"sotu_summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLLECTION_IF_EXISTS=True\n",
    "if DROP_COLLECTION_IF_EXISTS:\n",
    "    for collection_name in [page_content_collection_name, summary_collection_name]:\n",
    "        utility.drop_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content_collection = Collection(\n",
    "    name=page_content_collection_name,\n",
    "    schema=page_content_schema,\n",
    "    using='default',\n",
    "    shards_num=1\n",
    ")\n",
    "summary_collection = Collection(\n",
    "    name=summary_collection_name,\n",
    "    schema=summary_schema,\n",
    "    using='default',\n",
    "    shards_num=1\n",
    ")\n",
    "\n",
    "## TO GET EXISTING COLLECTIONS\n",
    "# page_content_collection = Collection(page_content_collection_name)\n",
    "# summary_collection = Collection(summary_collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings for the \"page_content\" and \"summary\" fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text:str, embedding_model=embeddings):\n",
    "    return embedding_model.embed_documents(text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in milvus_docs:\n",
    "    for field in [\"page_content\", \"summary\"]:\n",
    "        doc[f\"{field}_vector\"] = generate_embeddings(doc[field])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Documents to Milvus Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(insert count: 10, delete count: 10, upsert count: 10, timestamp: 444901700774658051, success count: 10, err count: 0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_content_collection.upsert(milvus_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(insert count: 10, delete count: 10, upsert count: 10, timestamp: 444901700774658059, success count: 10, err count: 0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_collection.upsert(milvus_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Vectors in Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = {\n",
    "  \"metric_type\":\"L2\",\n",
    "  \"index_type\":\"IVF_FLAT\",\n",
    "  \"params\":{\"nlist\":1024}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status(code=0, message=)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_content_collection.create_index(\n",
    "  field_name=\"page_content_vector\", \n",
    "  index_params=index_params\n",
    ")\n",
    "\n",
    "summary_collection.create_index(\n",
    "  field_name=\"summary_vector\", \n",
    "  index_params=index_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_rows': 0, 'indexed_rows': 0, 'pending_index_rows': 0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility.index_building_progress(\"sotu_text\")\n",
    "# Output: {'total_rows': 0, 'indexed_rows': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content_collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = {\n",
    "    \"metric_type\": \"L2\", \n",
    "    # \"offset\": 5, \n",
    "    \"ignore_growing\": False, \n",
    "    \"params\": {\"nprobe\": 10}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_vector = embeddings.embed_documents(\"Cancer Moonshot initiative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = page_content_collection.search(\n",
    "    data=search_vector, \n",
    "    anns_field=\"page_content_vector\", \n",
    "    # the sum of `offset` in `param` and `limit` \n",
    "    # should be less than 16384.\n",
    "    param=search_params,\n",
    "    limit=2,\n",
    "    expr=None,\n",
    "    # set the names of the fields you want to \n",
    "    # retrieve from the search result.\n",
    "    output_fields=['page_content'],\n",
    "    consistency_level=\"Strong\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id: 2, distance: 2.0121756908256239e-13, entity: {'page_content': 'Cancer is the #2 cause of death in Americaâ€“second only to heart disease. \\n\\nLast month, I announced our plan to supercharge  \\nthe Cancer Moonshot that President Obama asked me to lead six years ago. \\n\\nOur goal is to cut the cancer death rate by at least 50% over the next 25 years, turn more cancers from death sentences into treatable diseases.  \\n\\nMore support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health.'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BELOW STILL IN PROGRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval with Reranking (Boosting with Text Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenSearch(\n",
    "    hosts=OPENSEARCH_HTTPS_URL,\n",
    "    http_auth=(OPENSEARCH_USER, OPENSEARCH_PASSWORD),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assety_hostname=False,\n",
    "    ssl_show_warn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_query = {\n",
    "  'size': 5,\n",
    "  'query': {\n",
    "    'multi_match': {\n",
    "      'query': query,\n",
    "    #   'fields': ['title^2', 'director']\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_res = client.search(os_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os_res[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
